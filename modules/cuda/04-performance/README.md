# Модуль 4 — Оптимизация производительности (Shared Memory, Tiling)

Продолжаем тему матричного умножения и показываем, как использование *shared memory* и тайлов значительно ускоряет вычисления.

## Пример `matmul_tiled.cu`

– Тайловое ядро 16×16 с кэшированием блоков A и B в shared memory.  
– Измерение ускорения относительно наивного GEMM из Модуля 3.  
– Проверка корректности (для N ≤ 512).

### Сборка и запуск

```bash
cmake --build build --target matmul_tiled -j$(nproc)
./build/matmul_tiled           # 512×512
./build/matmul_tiled 1024      # 1024×1024
```

Ожидайте 5–20× ускорения по сравнению с `matmul_naive` (зависит от GPU).

## Задания

1. Измените `TILE` на 8, 16, 32 и измерьте время.
2. Используйте `-Xptxas -dlcm=ca` и посмотрите влияние кэш-политики.
3. Реализуйте версию с `WMMA` (Tensor Cores) и сравните производительность.

## Теория

Shared memory — это скоростная SRAM (≈70 TB/s на A100) размером 64-128 KB на SM. При тайловом GEMM блок нитей загружает подматрицы A и B в shared memory и переиспользует их `TILE` раз, снижая трафик к DRAM.

Формула операций/байт (operational intensity):
```
naive:    FLOPs / bytes ≈ 2N^3 / (3N^2 * sizeof(float)) ≈ 2 / (3*4)  ≈ 0.17
shared:   FLOPs / bytes ≈ 2N^3 / ( (N^2/TILE) * sizeof(float) )  ↑ во столько же раз
```

Чем выше intensity, тем ближе мы к теоретическому **peak FLOPs** (roofline-модель).

Банковские конфликты: shared memory разбита на 32 банка; доступ нитей варпа к разным банковым адресам выполняется параллельно. При обращении двух нитей к одному банку возникает конфликт ⇒ сериализация.

> Nsight Compute: метрика `shared_load_transactions_per_request` помогает выявить конфликты.

Tensor Cores (WMMA) позволяют обрабатывать на каждом такте матрицы 16×16×16; при этом требуется FP16/BF16 данные. В Модуле 12 будет пример WMMA. 